{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Interface for controlling the preporcessing part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "1. Make variable summarizations for both datasets\n",
    "2. send summarizations to lilian and dana\n",
    "3. create two datbase tables including indices and push both tables in the database\n",
    "4. write interface to use a list of variables, construct a query and gives back the resulting pandas frame\n",
    "5. test the interface\n",
    "6. data wrangling methods for pivoting, remove outliers, and data aggregation\n",
    "7. provide interface with sample query and sample preprocessing in a separte notebook\n",
    "8. document and cleanUp\n",
    "9. send to niklas, merge to main branch\n",
    "10. discuss further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to github repo:\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "'''\n",
    "%load_ext lab_black\n",
    "%matplotlib inline\n",
    "'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "\n",
    "from data_io import File_IO, Database_IO\n",
    "from data_wrangling import  Data_Wrangling\n",
    "from data_summarization import Data_Summarization\n",
    "from setup_config import Setup_Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = Setup_Config('config.ini') # loads a setup file with variables in .ini format\n",
    "# the .ini file contains e.g. database connections and other settings.\n",
    "#### ATTENTION: make sure to add this file to successfully to .gitignore to make sure it dont become pushed to the public repo.\n",
    "\n",
    "file_io = File_IO() # handles input and output operations\n",
    "\n",
    "data_wrangling = Data_Wrangling(data_io=file_io) # includes all the transformations\n",
    "data_summarizations = Data_Summarization(data_io=file_io)\n",
    "\n",
    "\n",
    "\n",
    "# if db not usage not wished, unccoment temporarly\n",
    "database_io = Database_IO(\n",
    "    host_ip=setup.config.db.host_ip, \n",
    "    port=setup.config.db.port, \n",
    "    db_user=setup.config.db.db_user, \n",
    "    db_pw=setup.config.db.db_pw, \n",
    "    db_name=setup.config.db.db_name) # use default postgres db\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge data in one csv file per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach: do it patient wise (unpack ziped files in a temp folder, summarize them, delete temp folder) -> saves storage, makes it work stepwise\n",
    "\n",
    "# the origin folder with the unprocessed, raw data\n",
    "data_set_name = 'n=101_OPENonOH_07.07.2022'\n",
    "folder_raw_data =  os.path.join(setup.config.files.data_root, *['raw', data_set_name])\n",
    "folder_patients = file_io.getSubfolders(folder_raw_data)\n",
    "\n",
    "# the folder to save the preprocessed csv files\n",
    "folder_merged_data =  os.path.join(setup.config.files.data_root, *['merged', data_set_name])\n",
    "\n",
    "# a temporary directory to unzip data in without modifieng the raw dataset\n",
    "folder_temp = os.path.join(setup.config.files.data_root, *['temp'])\n",
    "\n",
    "allready_done_ids = [ file_io.get_file_or_folder_name(path=filename).split('.csv')[0] \n",
    "    for filename in file_io.find_files_with_ending(folder_merged_data, ending='.csv') ]\n",
    "\n",
    "for patient_folder in folder_patients:\n",
    "\n",
    "    patient_id =  file_io.get_file_or_folder_name(path=patient_folder)\n",
    "    if patient_id in allready_done_ids: continue\n",
    "\n",
    "    print(patient_id)\n",
    "\n",
    "    # 1. unzip all gz, tar, zip files and copy to a temp dir    \n",
    "    file_io.unzip_recursively(root_path=patient_folder, root_path_copy=folder_temp)\n",
    "    \n",
    "    # 2. now get all json files within the temp dir\n",
    "    json_files = file_io.find_files_with_ending(path=folder_temp, ending='.json')\n",
    "\n",
    "    # 3. Merge all json files in one file of predefined format\n",
    "    #folder_name = patient_prefix + file_io.get_file_or_folder_name(path=patient_folder)\n",
    "    patients_jsons_dict = {patient_id : json_files} # use the patient_id as the folder name\n",
    "    #print(patients_jsons_dict)\n",
    "\n",
    "    if not os.path.exists(folder_merged_data): os.makedirs(folder_merged_data)  # if not exists create dir before\n",
    "    \n",
    "    # the outpath becomes combined with the patients identifier passed as key to the dict\n",
    "    # we discovered, that the startdate and timeasseconds are massively underpresented and drop them in the first run\n",
    "    # code could also be modified in a way to include them in key value store format like the other values\n",
    "    data_wrangling.group_patients_csv_data(patients_jsons_dict=patients_jsons_dict, outpath=folder_merged_data, \n",
    "                                            number_threads = 1, column_drop_list = ['startdate', 'timeasseconds']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frame[~frame[['startdate','timeasseconds']].isnull().any(axis=1)]\n",
    "#frame[~frame[['starttime']].isnull().any(axis=1)]\n",
    "#frame['timeasseconds'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Summarization of data (based on merged csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. use the merged files in predefined csv format calculate some statistics\n",
    "\n",
    "folder = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/data/merged/n=101_OPENonOH_07.07.2022'\n",
    "\n",
    "file_names = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "# summarization part in files\n",
    "#outpath='/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations'\n",
    "#data_summarizations.summarize_data(csv_files=file_names, outpath=outpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Insert data in postgres database\n",
    "Before a postgres db must be created on a server or localhost.\n",
    "The database must be specified in the config.ini file then.\n",
    "Test and activate the database connection in the settings section above.\n",
    "Assuming the database connection is working properbly, the following code creates new database tables, indices and inserts csv data in the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. insert data from csv files in new database table\n",
    "\n",
    "# 1. get csv file paths\n",
    "# 2. create new db table\n",
    "# 2b optional check if columns are in all csv files the same as well as the type ...\n",
    "# 3. insert data from all csv files recognized in the given file path to db\n",
    "\n",
    "\n",
    "# 1. get csv file paths\n",
    "file_paths = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "# choose a name for the target table, where the data will be saved (in current schema of the database specified in the config.ini file)\n",
    "target_table_name = 'open_uploaded_all'\n",
    "\n",
    "# 2. create new db table\n",
    "### creating a new database table, assuming typs are the same in all frames\n",
    "\n",
    "# create statically saves some time instead of dynamically getting column types\n",
    "#first_frame = file_io.read_csv_pandas(file_paths[0])\n",
    "#type_dict = database_io.get_df_column_types(first_frame) # dictionary of recognized columns and types\n",
    "\n",
    "# path,value,value_str,starttime,startdate,duration,isValid,timeasseconds,patient_id\n",
    "target_typ_dict= {\n",
    "    'path': str,\n",
    "    'value': float,\n",
    "    'value_str': str,\n",
    "    'starttime': datetime.datetime,\n",
    "    'startdate': datetime.datetime,\n",
    "    'duration': float,\n",
    "    'isValid': bool,\n",
    "    'timeasseconds' : int,\n",
    "    'patient_id': int\n",
    "}\n",
    "\n",
    "database_io.create_new_db_table(table_name=target_table_name, type_dict=target_typ_dict)\n",
    "\n",
    "# creates indices, which enable faster filtering of the database tables, e.g. with sql commands\n",
    "# TODO: If this function runs again, delete existsing index and overwrite. Currently we have to delete the db table manually before or recreate the index.\n",
    "database_io.create_new_index(table_name=target_table_name, index_name='open_uploaded_all_path', index_cols=['path'] )\n",
    "database_io.create_new_index(table_name=target_table_name, index_name='open_uploaded_all_patient_id', index_cols=['patient_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "def convert_dataframe_type(dataframe, target_typ_dict):\n",
    "    \"\"\" Converts the columns of a pandas dataframe to the specified types.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The pandas dataframe to be converted.\n",
    "        target_typ_dict (dict): The dictionary containing the target column type information.\n",
    "        Custom procedure for datetime objects.\n",
    "    \"\"\"\n",
    "    for col, typ in target_typ_dict.items():\n",
    "        #dataframe[col] = dataframe[col].astype(typ) if typ != datetime.datetime else pd.to_datetime(dataframe[col])\n",
    "        #dataframe[col] = dataframe[col].replace([np.inf, -np.inf], None).astype(typ) if typ != datetime.datetime else pd.to_datetime(dataframe[col])\n",
    "\n",
    "        dataframe[col] = dataframe[col].replace((np.nan, ''), (None, None)).astype(typ) if typ != datetime.datetime else pd.to_datetime(dataframe[col])\n",
    "        \n",
    "    return dataframe\n",
    "'''\n",
    "\n",
    "\n",
    "def convert_datatypes(target_typ_dict, df):    \n",
    "    \"\"\"\n",
    "    This function converts the columns of a given pandas dataframe to their corresponding datatypes, \n",
    "    as specified in a given target_typ_dict. If a conversion is not possible, the value is replaced by None.\n",
    "    \"\"\"\n",
    "    for key, value in target_typ_dict.items():\n",
    "        if value == str:\n",
    "            df[key] = df[key].astype(str)\n",
    "        elif value == float:\n",
    "            df[key] = df[key].astype(float)\n",
    "        elif value == datetime.datetime:\n",
    "            df[key] = pd.to_datetime(df[key], errors=\"coerce\")\n",
    "        elif value == bool:\n",
    "            df[key] = df[key].astype(bool)\n",
    "        elif value == int:\n",
    "            df[key] = pd.to_numeric(df[key], downcast=\"integer\", errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = file_io.read_csv_pandas(file_paths[0])\n",
    "first_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = convert_datatypes(target_typ_dict, first_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head(5) # check if it is has worked properbly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data to database\n",
    "database_io.append_frame_to_sql_table(table_name='open_uploaded_all',data=frame.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. insert data from all csv files recognized in the given file path to db\n",
    "'''\n",
    "for file_path in file_paths:\n",
    "    frame = file_io.read_csv_pandas(file_path)\n",
    "    database_io.append_frame_to_sql_table(table_name=target_table_name,data=frame)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    frame = file_io.read_csv_pandas(file_path)\n",
    "    type_dict = database_io.get_df_column_types(first_frame)\n",
    "    print(type_dict)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TODO: merge the summarization files to one summarization file\n",
    "\n",
    "folder = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations'\n",
    "file_paths = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "frames = []\n",
    "for file_path in file_paths: frames.append( file_io.read_csv_pandas(file_path) )\n",
    "merged_frame = data_summarizations.merge_pandas_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath='/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations/summarization.csv'\n",
    "file_io.pandas_to_csv(frame=merged_frame, outpath=outpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is next?\n",
    "\n",
    "In the next additional notebook we provide an user interface to efficiently use the database in order to filter the required paths out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. database setup\n",
    "\n",
    "# Next TODO s: Creeate postgres DB (Done)\n",
    "# Implement data io to read/write from to db\n",
    "# filter and insert data into db\n",
    "# create indices to optimize performance and optimize docker postgres performance\n",
    "\n",
    "# provide an interface to access the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Filter relevant data based on txt file path selection and insert in postgres db table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. use csv file or database to access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. transform table in pivoted cleaned and accessable table (in postgres with login)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Code snipts here are work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. merge the summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_stats = merge_csv_files(stat_file_names)\n",
    "#merged_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playgorund - Chat GPT Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "aggregation = {\n",
    "    \"value_count\": \"sum\", \n",
    "    \"mean\": lambda x: np.sum(x)/np.sum(merged_stats[\"value_count\"]),\n",
    "    \"max\": \"max\",\n",
    "    \"min\": \"min\",\n",
    "    \"mean_duration\": \"mean\"\n",
    "}\n",
    "\n",
    "df_aggregated = merged_stats.groupby(\"path\").agg(aggregation)\n",
    "df_aggregated\n",
    "\n",
    "#return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group by path and aggregate\n",
    "out = df.groupby('path').agg({'value_count': 'sum', 'mean': lambda x: sum(x)/sum(df['value_count']),\n",
    "                              'max': 'max', 'min': 'min', 'mean_duration': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define list of file names\n",
    "csv_files = ['file1.csv', 'file2.csv', 'file3.csv', 'file4.csv']\n",
    "\n",
    "# create empty dataframe\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# loop through files\n",
    "for csv in csv_files:\n",
    "    # read csv\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # calculate aggregates\n",
    "    df['mean'] = df['mean'] * df['value_count']\n",
    "    df['min'] = df['min'] * df['value_count']\n",
    "    df['max'] = df['max'] * df['value_count']\n",
    "    df['mean_duration'] = df['mean_duration'] * df['value_count']\n",
    "    \n",
    "    # group by path\n",
    "    df_grouped = df.groupby('path').sum()\n",
    "    \n",
    "    # calculate new aggregates\n",
    "    df_grouped['mean'] = df_grouped['mean'] / df_grouped['value_count']\n",
    "    df_grouped['min'] = df_grouped['min'] / df_grouped['value_count']\n",
    "    df_grouped['max'] = df_grouped['max'] / df_grouped['value_count']\n",
    "    df_grouped['mean_duration'] = df_grouped['mean_duration'] / df_grouped['value_count']\n",
    "    \n",
    "    # append to merged dataframe\n",
    "    merged_df = pd.concat([merged_df, df_grouped])\n",
    "    \n",
    "# drop duplicates and reset index\n",
    "merged_df = merged_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# display dataframe\n",
    "print(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csp",
   "language": "python",
   "name": "csp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
