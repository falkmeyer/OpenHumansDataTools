{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Interface for controlling the preprcessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source /Users/me/Desktop/Code/venvs/open_diabetis/bin/activate\n",
    "\n",
    "\n",
    "# Link to github repo:\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "\n",
    "'''\n",
    "%load_ext lab_black\n",
    "%matplotlib inline\n",
    "'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "\n",
    "from data_io import File_IO, Database_IO\n",
    "from data_wrangling import  Data_Wrangling\n",
    "from data_summarization import Data_Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_io = File_IO() # handles input and output operations\n",
    "\n",
    "data_wrangling = Data_Wrangling(data_io=file_io) # includes all the transformations\n",
    "data_summarizations = Data_Summarization(data_io=file_io)\n",
    "\n",
    "# if db not usage not wished, unccoment temporarly\n",
    "database_io = Database_IO(\n",
    "    host_ip='81.169.210.147', \n",
    "    port=5849, \n",
    "    db_user='postgres', \n",
    "    db_pw='jhg23675sdfjhg276', \n",
    "    db_name='postgres') # use default postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in all json files for each patient\n",
    "\n",
    "parent_folder = \"/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/data\" # if not exists create dir before\n",
    "patients_jsons_dict = file_io.get_dict_of_folder_and_json_files(parent_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. merge all json files in one csv for each patient with columns:: path, is_valid, starttime, value, value_str\n",
    "\n",
    "outpath = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/data' # if not exists create dir before\n",
    "data_wrangling.group_patients_csv_data(patients_jsons_dict, outpath=outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. use the merged files in predefined csv format calculate some statistics\n",
    "\n",
    "folder = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/data'\n",
    "\n",
    "file_names = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "outpath='/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations'\n",
    "data_summarizations.summarize_data(csv_files=file_names, outpath=outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. insert data from csv files in new database table\n",
    "\n",
    "# 1. get csv file paths\n",
    "# 2. create new db table\n",
    "# 2b optional check if columns are in all csv files the same as well as the type ...\n",
    "# 3. insert data from all csv files recognized in the given file path to db\n",
    "\n",
    "file_paths = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    frame = file_io.read_csv_pandas(file_path)\n",
    "    \n",
    "    \n",
    "    print(frame.head(5))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TODO: merge the summarizations to one summarization file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. database setup\n",
    "\n",
    "# Next TODO s: Creeate postgres DB (Done)\n",
    "# Implement data io to read/write from to db\n",
    "# filter and insert data into db\n",
    "# create indices to optimize performance and optimize docker postgres performance\n",
    "\n",
    "# provide an interface to access the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Filter relevant data based on txt file path selection and insert in postgres db table\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. use csv file or database to access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. transform table in pivoted cleaned and accessable table (in postgres with login)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Code snipts here are work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. merge the summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_stats = merge_csv_files(stat_file_names)\n",
    "#merged_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playgorund - Chat GPT Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "aggregation = {\n",
    "    \"value_count\": \"sum\", \n",
    "    \"mean\": lambda x: np.sum(x)/np.sum(merged_stats[\"value_count\"]),\n",
    "    \"max\": \"max\",\n",
    "    \"min\": \"min\",\n",
    "    \"mean_duration\": \"mean\"\n",
    "}\n",
    "\n",
    "df_aggregated = merged_stats.groupby(\"path\").agg(aggregation)\n",
    "df_aggregated\n",
    "\n",
    "#return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group by path and aggregate\n",
    "out = df.groupby('path').agg({'value_count': 'sum', 'mean': lambda x: sum(x)/sum(df['value_count']),\n",
    "                              'max': 'max', 'min': 'min', 'mean_duration': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define list of file names\n",
    "csv_files = ['file1.csv', 'file2.csv', 'file3.csv', 'file4.csv']\n",
    "\n",
    "# create empty dataframe\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# loop through files\n",
    "for csv in csv_files:\n",
    "    # read csv\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # calculate aggregates\n",
    "    df['mean'] = df['mean'] * df['value_count']\n",
    "    df['min'] = df['min'] * df['value_count']\n",
    "    df['max'] = df['max'] * df['value_count']\n",
    "    df['mean_duration'] = df['mean_duration'] * df['value_count']\n",
    "    \n",
    "    # group by path\n",
    "    df_grouped = df.groupby('path').sum()\n",
    "    \n",
    "    # calculate new aggregates\n",
    "    df_grouped['mean'] = df_grouped['mean'] / df_grouped['value_count']\n",
    "    df_grouped['min'] = df_grouped['min'] / df_grouped['value_count']\n",
    "    df_grouped['max'] = df_grouped['max'] / df_grouped['value_count']\n",
    "    df_grouped['mean_duration'] = df_grouped['mean_duration'] / df_grouped['value_count']\n",
    "    \n",
    "    # append to merged dataframe\n",
    "    merged_df = pd.concat([merged_df, df_grouped])\n",
    "    \n",
    "# drop duplicates and reset index\n",
    "merged_df = merged_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# display dataframe\n",
    "print(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charite-projman",
   "language": "python",
   "name": "charite-projman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecfad3485449a33caf4f3f5e69adc661d50e7627e682f34938f9c3de629b56dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
