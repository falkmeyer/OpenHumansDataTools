{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook-Interface for controlling the preprcessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source /Users/me/Desktop/Code/venvs/open_diabetis/bin/activate\n",
    "\n",
    "\n",
    "# Link to github repo:\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "\n",
    "'''\n",
    "%load_ext lab_black\n",
    "%matplotlib inline\n",
    "'''\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "\n",
    "from data_io import File_IO, Database_IO\n",
    "from data_wrangling import  Data_Wrangling\n",
    "from data_summarization import Data_Summarization\n",
    "from setup_config import Setup_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = Setup_Config('config.ini') # loads a setup file with variables in .ini format\n",
    "# the .ini file contains e.g. database connections and other settings.\n",
    "#### ATTENTION: make sure to add this file to successfully to .gitignore to make sure it dont become pushed to the public repo.\n",
    "\n",
    "file_io = File_IO() # handles input and output operations\n",
    "\n",
    "data_wrangling = Data_Wrangling(data_io=file_io) # includes all the transformations\n",
    "data_summarizations = Data_Summarization(data_io=file_io)\n",
    "\n",
    "'''\n",
    "\n",
    "# if db not usage not wished, unccoment temporarly\n",
    "database_io = Database_IO(\n",
    "    host_ip=setup.config.db.host_ip, \n",
    "    port=setup.config.db.port, \n",
    "    db_user=setup.config.db.db_user, \n",
    "    db_pw=setup.config.db.db_pw, \n",
    "    db_name=setup.config.db.db_name) # use default postgres db'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in all json files for each patient\n",
    "\n",
    "parent_folder = \"/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/data\" # if not exists create dir before\n",
    "patients_jsons_dict = file_io.get_dict_of_folder_and_json_files(parent_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. merge all json files in one csv for each patient with columns:: path, is_valid, starttime, value, value_str\n",
    "\n",
    "outpath = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/data' # if not exists create dir before\n",
    "data_wrangling.group_patients_csv_data(patients_jsons_dict, outpath=outpath)\n",
    "\n",
    "\n",
    "# TODO: which time zone is used in uploaded data. where to find the timezone\n",
    "# there is a file called UploadInfo, which provides an utc offset. \n",
    "# calculate the utc offset on the unix data and convert to python date format\n",
    "# if somebody switches timezones, we have to capture this and check for each upload seperatly\n",
    "\n",
    "# the utc time is meassured in miliseconds, as well as the utc offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. use the merged files in predefined csv format calculate some statistics\n",
    "\n",
    "folder = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/data'\n",
    "\n",
    "file_names = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "outpath='/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations'\n",
    "data_summarizations.summarize_data(csv_files=file_names, outpath=outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. insert data from csv files in new database table\n",
    "\n",
    "# 1. get csv file paths\n",
    "# 2. create new db table\n",
    "# 2b optional check if columns are in all csv files the same as well as the type ...\n",
    "# 3. insert data from all csv files recognized in the given file path to db\n",
    "\n",
    "\n",
    "# 1. get csv file paths\n",
    "file_paths = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "# choose a name for the target table, where the data will be saved (in current schema of the database specified in the config.ini file)\n",
    "target_table_name = 'open_uploaded_all'\n",
    "\n",
    "# 2. create new db table\n",
    "### creating a new database table, assuming typs are the same in all frames\n",
    "first_frame = file_io.read_csv_pandas(file_paths[0])\n",
    "type_dict = database_io.get_df_column_types(first_frame) # dictionary of recognized columns and tyoes\n",
    "database_io.create_new_db_table(table_name=target_table_name, type_dict=type_dict)\n",
    "\n",
    "# creates indices, which enable faster filtering of the database tables, e.g. with sql commands\n",
    "# TODO: If this function runs again, delete existsing index and overwrite. Currently we have to delete the db table manually before or recreate the index.\n",
    "database_io.create_new_index(table_name=target_table_name, index_name='open_uploaded_all_path', index_cols=['path'] )\n",
    "database_io.create_new_index(table_name=target_table_name, index_name='open_uploaded_all_patient_id', index_cols=['patient_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_typ_dict= {\n",
    "    'path': str,\n",
    "    'value': float,\n",
    "    'value_str': str,\n",
    "    'starttime': pd._libs.tslibs.timestamps.Timestamp,\n",
    "    'duration': float,\n",
    "    'isValid': bool,\n",
    "    'patient_id': int\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_type(dataframe, target_typ_dict):\n",
    "    \"\"\" Converts the columns of a pandas dataframe to the specified types.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The pandas dataframe to be converted.\n",
    "        target_typ_dict (dict): The dictionary containing the target column type information.\n",
    "    \"\"\"\n",
    "    for col, typ in target_typ_dict.items():\n",
    "        dataframe[col] = dataframe[col].astype(typ)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dataframe_type(dataframe=test_frame, target_typ_dict=target_typ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = file_io.read_csv_pandas(file_paths[0])\n",
    "test_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. insert data from all csv files recognized in the given file path to db\n",
    "\n",
    "for file_path in file_paths:\n",
    "    frame = file_io.read_csv_pandas(file_path)\n",
    "    database_io.append_frame_to_sql_table(table_name=target_table_name,data=frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_paths:\n",
    "    frame = file_io.read_csv_pandas(file_path)\n",
    "    type_dict = database_io.get_df_column_types(first_frame)\n",
    "    print(type_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TODO: merge the summarization files to one summarization file\n",
    "\n",
    "folder = '/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations'\n",
    "file_paths = file_io.get_csv_files(folder=folder)\n",
    "\n",
    "frames = []\n",
    "for file_path in file_paths: frames.append( file_io.read_csv_pandas(file_path) )\n",
    "merged_frame = data_summarizations.merge_pandas_frames(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath='/Users/me/Desktop/Code/VS-Projects/charite/OpenHumansDataTools/temp/summarizations/summarization.csv'\n",
    "file_io.pandas_to_csv(frame=merged_frame, outpath=outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. database setup\n",
    "\n",
    "# Next TODO s: Creeate postgres DB (Done)\n",
    "# Implement data io to read/write from to db\n",
    "# filter and insert data into db\n",
    "# create indices to optimize performance and optimize docker postgres performance\n",
    "\n",
    "# provide an interface to access the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Filter relevant data based on txt file path selection and insert in postgres db table\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. use csv file or database to access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. transform table in pivoted cleaned and accessable table (in postgres with login)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Code snipts here are work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. merge the summarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_stats = merge_csv_files(stat_file_names)\n",
    "#merged_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playgorund - Chat GPT Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "aggregation = {\n",
    "    \"value_count\": \"sum\", \n",
    "    \"mean\": lambda x: np.sum(x)/np.sum(merged_stats[\"value_count\"]),\n",
    "    \"max\": \"max\",\n",
    "    \"min\": \"min\",\n",
    "    \"mean_duration\": \"mean\"\n",
    "}\n",
    "\n",
    "df_aggregated = merged_stats.groupby(\"path\").agg(aggregation)\n",
    "df_aggregated\n",
    "\n",
    "#return df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group by path and aggregate\n",
    "out = df.groupby('path').agg({'value_count': 'sum', 'mean': lambda x: sum(x)/sum(df['value_count']),\n",
    "                              'max': 'max', 'min': 'min', 'mean_duration': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define list of file names\n",
    "csv_files = ['file1.csv', 'file2.csv', 'file3.csv', 'file4.csv']\n",
    "\n",
    "# create empty dataframe\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# loop through files\n",
    "for csv in csv_files:\n",
    "    # read csv\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # calculate aggregates\n",
    "    df['mean'] = df['mean'] * df['value_count']\n",
    "    df['min'] = df['min'] * df['value_count']\n",
    "    df['max'] = df['max'] * df['value_count']\n",
    "    df['mean_duration'] = df['mean_duration'] * df['value_count']\n",
    "    \n",
    "    # group by path\n",
    "    df_grouped = df.groupby('path').sum()\n",
    "    \n",
    "    # calculate new aggregates\n",
    "    df_grouped['mean'] = df_grouped['mean'] / df_grouped['value_count']\n",
    "    df_grouped['min'] = df_grouped['min'] / df_grouped['value_count']\n",
    "    df_grouped['max'] = df_grouped['max'] / df_grouped['value_count']\n",
    "    df_grouped['mean_duration'] = df_grouped['mean_duration'] / df_grouped['value_count']\n",
    "    \n",
    "    # append to merged dataframe\n",
    "    merged_df = pd.concat([merged_df, df_grouped])\n",
    "    \n",
    "# drop duplicates and reset index\n",
    "merged_df = merged_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# display dataframe\n",
    "print(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charite-projman",
   "language": "python",
   "name": "charite-projman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecfad3485449a33caf4f3f5e69adc661d50e7627e682f34938f9c3de629b56dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
